---
layout: page
title: EmoDB Speech Classification
subtitle: 
bigimg: /img/speech.jpg
---

<p align="right">
<br>You can access the presentation <a href="https://drive.google.com/file/d/10l_md1TePen57xGRHkdcrL_OFZCpGEEy/view?usp=sharing"> here </a><br>and the github repo 
<a href="https://github.com/ymentha14/EmoDB/settings"> here </a>
</p>

## Introduction
<a href="http://emodb.bilderbar.info/docu/#download">EmoDB</a>, as known as Berlin Database of Emotional Speech, is a dataset presenting short speeches of 6 types of emotions in german. *(anger, happiness, fear, disgust, neutral and boredom)* The project, made in the context of a one-week hackathon, consisted of the following goals:
* Classifying the emotions with highest accuracy
* Develop a REST API to serve the results
* Dockerize the whole application

## Implementation
* As a preprocessing step, all mp3 files were converted to their respective mfcc features, the state of the art ones for speech classification
* The best performing model is a sequential CNN with classic batchnorm2d and dropout features, inspired from <a href="https://www.researchgate.net/publication/338138024_Emotion_Recognition_from_Speech"> this paper. </a>
* Various types of Data augmentation (temporal shift, gaussian noise addition, pitch tuning) alowed to obtain a gain of **10% on validation loss** (3 train/val cv, 15 epochs)

## Results
<img src= "/img/speech_result1.png">
<img src= "/img/speech_result2.png">
A train accuracy of 98 +- 2.3 % and a validation of 65 +- 3.7% were obtained, witnessing of the need of other regularization methods.


## Retrospective Improvements

Ideally, LSTM and RNN more generally would be suitable architectures to explore due to the sequential nature of the data. With enough training, attention models could lead to interesting outcomes too. Finally , some other data augmentation could as well increase the validation accuracy, such as random cropping/acceleration deceleration.

