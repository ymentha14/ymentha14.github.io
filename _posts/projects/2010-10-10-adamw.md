---
layout: page
title: From Adam to W
bigimg: /img/adamw.png
---
<p align="right">
<i>In collaboration with Jonathan Besomi and Stefano Huber</i><br>You can access the full data story <a href="https://drive.google.com/file/d/1pAmokAby9SHlZAhV0zT6Fft69-jq3gVP/view?usp=sharing"> here </a><br>
<a href="https://github.com/ymentha14/FromAdamtoW">Github Repo </a>
<a href="https://www.fast.ai/2018/07/02/adam-weight-decay/">AdamW reference article </a>
</p>
## Introduction
The project consists in an overview and performance comparison of the AdamW optimizer over the classically used Adam on 3 state of the art datasets in order to assess significant improvement in a practical application.

## Personal Involvement

* Development of the gridsearch framework (pytorch)
In order to assess the impact of AdamW over Adam, we needed to optimize both frameworks evenly. A classical gridsearch implementation was made to tackle this challenge.
* Development of the speech classification model (pytorch)
One of the three state-of-the art model consisted of a speech classification task for which I implemented a deep convolutionnal network.
* Statistical Regression models
A t-test was implemented to segregate between the mean accuracy/loss curves of the Adam optimizer and the AdamW ones. These curves showed statistical significance in a few cases.

